package main

import (
	"bufio"
	"compress/gzip"
	"errors"
	"flag"
	"fmt"
	"io"
	"io/ioutil"
	"log"
	"math/rand"
	"net/http"
	"os"
	"path/filepath"
	"runtime"
	"runtime/pprof"
	"strings"
	"sync"
	"time"

	"github.com/miku/esbulk"
	"github.com/sethgrid/pester"
)

// Version of application.
const Version = "0.5.1"

// indexSettingsRequest runs updates an index setting, given a body and
// options. Body consist of the JSON document, e.g. `{"index":
// {"refresh_interval": "1s"}}`.
func indexSettingsRequest(body string, options esbulk.Options) (*http.Response, error) {
	r := strings.NewReader(body)

	rand.Seed(time.Now().Unix())
	server := options.Servers[rand.Intn(len(options.Servers))]
	link := fmt.Sprintf("%s/%s/_settings", server, options.Index)

	req, err := http.NewRequest("PUT", link, r)
	if err != nil {
		return nil, err
	}
	// Auth handling.
	if options.Username != "" && options.Password != "" {
		req.SetBasicAuth(options.Username, options.Password)
	}
	req.Header.Set("Content-Type", "application/json")

	resp, err := pester.Do(req)
	if err != nil {
		return nil, err
	}
	if options.Verbose {
		log.Printf("applied setting: %s with status %s\n", body, resp.Status)
	}
	return resp, nil
}

func main() {
	var serverFlags esbulk.ArrayFlags
	flag.Var(&serverFlags, "server", "elasticsearch server, this works with https as well")
	batchSize := flag.Int("size", 1000, "bulk batch size")
	numWorkers := flag.Int("w", runtime.NumCPU(), "number of workers to use")
	verbose := flag.Bool("verbose", false, "output basic progress")
	gzipped := flag.Bool("z", false, "unzip gz'd file on the fly")
	mapping := flag.String("mapping", "", "mapping string or filename to apply before indexing")
	purge := flag.Bool("purge", false, "purge any existing index before indexing")
	idfield := flag.String("id", "", "name of field to use as id field, by default ids are autogenerated")
	user := flag.String("u", "", "http basic auth username:password, like curl -u")
	zeroReplica := flag.Bool("0", false, "set the number of replicas to 0 during indexing")
	sourceDir := flag.String("dir", "", "path to directory with source JSON documents")
	keepProcessed := flag.Bool("keep", true, "do not delete file from source directory after is processed(default is true)")

	flag.Parse()

	if *cpuprofile != "" {
		f, err := os.Create(*cpuprofile)
		if err != nil {
			log.Fatal(err)
		}
		pprof.StartCPUProfile(f)
		defer pprof.StopCPUProfile()
	}

	if *version {
		fmt.Println(Version)
		os.Exit(0)
	}

	if len(serverFlags) == 0 {
		serverFlags = append(serverFlags, "http://localhost:9200")
	}

	if *verbose {
		log.Printf("using %d servers", len(serverFlags))
	}

	runtime.GOMAXPROCS(*numWorkers)

	var username, password string
	if len(*user) > 0 {
		parts := strings.Split(*user, ":")
		if len(parts) != 2 {
			log.Fatal("http basic auth syntax is: username:password")
		}
		username = parts[0]
		password = parts[1]
	}

	defaultOptions := esbulk.Options{
		Servers:     serverFlags,
		Index:       *indexName,
		Purge:       *purge,
		Mapping:     *mapping,
		NumWorkers:  *numWorkers,
		ZeroReplica: *zeroReplica,
		GZipped:     *gzipped,
		DocType:     *docType,
		BatchSize:   *batchSize,
		Verbose:     *verbose,
		Scheme:      "http",
		IDField:     *idfield,
		Username:    username,
		Password:    password,
	}

	counter := 0
	start := time.Now()
	var reader io.Reader

	if *sourceDir != "" {
		// process files from source directory
		files, err := ioutil.ReadDir(*sourceDir)
		if err != nil {
			log.Fatalf("failed to list directory due to %s", err)
		}

		for _, file := range files {
			if file.IsDir() {
				continue
			}

			path := filepath.Join(*sourceDir, file.Name())

			options, err := parseLDJFile(path, defaultOptions)
			if err != nil {
				log.Print(err)
				continue
			}

			f, err := os.Open(path)
			defer f.Close()
			if err != nil {
				log.Print(err)
				continue
			}
			reader = f

			count, err := processLDJFile(reader, options)
			if err != nil {
				log.Print(err)
				continue
			}

			counter += count

			if !*keepProcessed {
				err := os.Remove(path)
				if err != nil {
					log.Print(err)
					continue
				}
			}

			if options.Verbose {
				log.Printf("finished file %q...", file.Name())
			}
		}
	} else {
		// process single file or STDIN
		reader = os.Stdin
		filename := ""
		if flag.NArg() > 0 {
			filename = flag.Arg(0)
			f, err := os.Open(filename)
			if err != nil {
				log.Fatalln(err)
			}
			defer f.Close()
			reader = f
		}

		count, err := processLDJFile(reader, defaultOptions)
		if err != nil {
			log.Fatal(err)
		}
		count += counter

		if !*keepProcessed && filename != "" {
			err := os.Remove(filename)
			if err != nil {
				log.Fatal(err)
			}
		}
	}

	elapsed := time.Since(start)

	if *memprofile != "" {
		f, err := os.Create(*memprofile)
		if err != nil {
			log.Fatal(err)
		}
		pprof.WriteHeapProfile(f)
		f.Close()
	}

	if *verbose {
		rate := float64(counter) / elapsed.Seconds()
		log.Printf("%d docs in %s at %0.3f docs/s with %d workers\n", counter, elapsed, rate, *numWorkers)
	}
}

func processLDJFile(r io.Reader, options esbulk.Options) (int, error) {
	count := 0

	if options.Index == "" {
		return count, errors.New("index name required")
	}

	if options.Verbose {
		log.Println(options)
	}

	if options.Purge {
		if err := esbulk.DeleteIndex(options); err != nil {
			return count, err
		}
		time.Sleep(5 * time.Second)
	}

	// Create index if not exists.
	if err := esbulk.CreateIndex(options); err != nil {
		return count, err
	}

	if options.Mapping != "" {
		var reader io.Reader
		if _, err := os.Stat(options.Mapping); os.IsNotExist(err) {
			reader = strings.NewReader(options.Mapping)
		} else {
			file, err := os.Open(options.Mapping)
			if err != nil {
				return count, err
			}
			reader = bufio.NewReader(file)
		}
		err := esbulk.PutMapping(options, reader)
		if err != nil {
			return count, err
		}
	}

	queue := make(chan string)
	var wg sync.WaitGroup

	for i := 0; i < options.NumWorkers; i++ {
		wg.Add(1)
		go esbulk.Worker(fmt.Sprintf("worker-%d", i), options, queue, &wg)
	}

	for i := range options.Servers {
		// Store number_of_replicas settings for restoration later.
		doc, err := esbulk.GetSettings(i, options)
		if err != nil {
			return count, err
		}

		// TODO(miku): Rework this.
		numberOfReplicas := doc[options.Index].(map[string]interface{})["settings"].(map[string]interface{})["index"].(map[string]interface{})["number_of_replicas"]
		if options.Verbose {
			log.Printf("on shutdown, number_of_replicas will be set back to %s", numberOfReplicas)
		}

		// Shutdown procedure. TODO(miku): Handle signals, too.
		defer func() {
			// Realtime search.
			if _, err := indexSettingsRequest(`{"index": {"refresh_interval": "1s"}}`, options); err != nil {
				log.Fatal(err)
			}
			// Reset number of replicas.
			if _, err := indexSettingsRequest(fmt.Sprintf(`{"index": {"number_of_replicas": %q}}`, numberOfReplicas), options); err != nil {
				log.Fatal(err)
			}

			// Persist documents.
			if err := esbulk.FlushIndex(i, options); err != nil {
				log.Fatal(err)
			}
		}()

		// Realtime search.
		resp, err := indexSettingsRequest(`{"index": {"refresh_interval": "-1"}}`, options)
		if err != nil {
			return count, err
		}
		if resp.StatusCode >= 400 {
			return count, fmt.Errorf("failed with response %v", resp)
		}
		if options.ZeroReplica {
			// Reset number of replicas.
			if _, err := indexSettingsRequest(`{"index": {"number_of_replicas": 0}}`, options); err != nil {
				return count, err
			}
		}
	}

	reader := bufio.NewReader(r)
	if options.GZipped {
		zreader, err := gzip.NewReader(r)
		if err != nil {
			return count, err
		}
		reader = bufio.NewReader(zreader)
	}

	for {
		line, err := reader.ReadString('\n')
		if err == io.EOF {
			break
		}
		if err != nil {
			return count, err
		}
		line = strings.TrimSpace(line)
		if len(line) == 0 {
			continue
		}
		queue <- line
		count++
	}

	close(queue)
	wg.Wait()

	return count, nil
}

func parseLDJFile(path string, defaults esbulk.Options) (esbulk.Options, error) {
	log.Printf("processing file %q...", path)
	if _, err := os.Stat(path); os.IsNotExist(err) {
		return esbulk.Options{}, fmt.Errorf("failed to load %q as it does not exists", path)
	}

	tokens := strings.Split(filepath.Base(path), ".")
	switch len(tokens) {
	case 4:
		// with id argument
		defaults.Index = tokens[2]
		defaults.DocType = tokens[1]
		defaults.IDField = tokens[0]
		return defaults, nil
	case 3:
		defaults.Index = tokens[1]
		defaults.DocType = tokens[0]
		return defaults, nil
	default:
		// no id argument
		return esbulk.Options{}, fmt.Errorf("failed to parse source LDL file %q", path)
	}
}
